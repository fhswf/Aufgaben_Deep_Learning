{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9c02df-4507-4656-9acb-f65043deebb4",
   "metadata": {},
   "source": [
    "# Aufgabe 1: Reinforcement Learning mit Policy Network\n",
    "\n",
    "![CartPole](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "[CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) ist ein Kontrollproblem, bei dem es darum geht, ein auf einem Wagen montiertes Pendel zu balancieren.\n",
    "\n",
    "Es eignet sich als einfaches Einstiegsbeispiel für Reinforcement Learning, da der Zustandsraum mit vier Dimensionen \n",
    "\n",
    "- Position des Wagens,\n",
    "- Geschwindigkeit des Wagens,\n",
    "- Winkel des Pendels, und\n",
    "- Winkelgeschwindigkeit des Pendels\n",
    "\n",
    "sehr klein und die Anzahl der Aktionen\n",
    "\n",
    "- Wagen nach links schieben oder\n",
    "- Wagen nach rechts schieben\n",
    "\n",
    "überschaubar ist.\n",
    "\n",
    "In der ersten Aufgabe geht es darum, das Pendel mit einem einfachen *Policy Network* auszubalancieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e8ecf4-4cee-454d-9671-9f10c8dfabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym) (2.2.0)\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.9 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.2\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.64.1)\n",
      "Collecting proglog<=1.0.0\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.21.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.22.4)\n",
      "Collecting imageio-ffmpeg>=0.2.0\n",
      "  Using cached imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.6.15.1)\n",
      "Installing collected packages: proglog, imageio-ffmpeg, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-ffmpeg-0.4.7 moviepy-1.0.3 proglog-0.1.10\n",
      "Collecting pysdl2\n",
      "  Using cached PySDL2-0.9.14-py3-none-any.whl\n",
      "Installing collected packages: pysdl2\n",
      "Successfully installed pysdl2-0.9.14\n",
      "Collecting pyvirtualdisplay\n",
      "  Using cached PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyvirtualdisplay\n",
      "Successfully installed pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install moviepy\n",
    "!pip install pysdl2\n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a5271-2699-4cac-8659-1b05d9eef977",
   "metadata": {},
   "source": [
    "Zunächst erzeugen wir ein *Environment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd42433-74be-4913-8fe5-13f8f58f2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3891a0-f9f4-453a-8f3c-c924b0e5c264",
   "metadata": {},
   "source": [
    "## Rendering im Jupyter Notebook\n",
    "\n",
    "Normalerweise benötigt man für das Rendern eine \"richtige\" Anwendung. Wir behelfen uns hier mit `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f6f9f5-9cb4-4fd1-b50e-3b610bb8c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bb37-6b05-4814-a415-717f428d7fdd",
   "metadata": {},
   "source": [
    "## Baseline: `RandomPolicy`\n",
    "\n",
    "Die folgende Policy macht einfach zufällige Aktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a857b711-b64f-4c46-bde6-cf1a85883316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy:\n",
    "    \n",
    "    def __call__(self, observation):\n",
    "        return random.choice([0, 1])\n",
    "    \n",
    "    def update(self, *args):\n",
    "        # Do nothing\n",
    "        pass\n",
    "    \n",
    "    def init_game(self, observation):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3d47b6-8d9f-4de3-bb1a-ab6186e48c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(policy, episodes=2000, do_render = False, seed=100):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if do_render:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    else:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "   \n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status, refresh=episode % 10 == 0)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "                    \n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b00b3cd-23d3-49ff-b0ff-460a24abba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39bd7777e0d4901bf3b2468fa0deb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = RandomPolicy()\n",
    "play_game(policy, episodes=10, do_render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac67e9-f594-467f-947e-88759c5d07a6",
   "metadata": {},
   "source": [
    "## Aufgabe 1.1: Policy Network\n",
    "\n",
    "Für unser erstes Model benötigen wir ein Policy-Network, das den vierdiemesionalen Zustand in zwei Aktionen übersetzt. \n",
    "Das Modell so wie folgt aussehen:\n",
    "\n",
    "1. Ein `Linear` Layer mit `hidden_size` als Zieldimension und `ReLU` als Aktivierungsfunktion,\n",
    "2. Als `policy_head` ein `Linear` Layer mit `n_actions` als Zieldimension und `Softmax` als Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25a7c1-da09-445e-8366-2a682e8c68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32, n_actions=2):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE HERE\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a80f57-6df6-4adf-9b81-ba440d6fcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob'])\n",
    "    \n",
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = PolicyNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ce84d-9406-40b0-acb8-44a7e9aa663e",
   "metadata": {},
   "source": [
    "### Strategie\n",
    "\n",
    "Die Funktion `__call__(self, observation)` berechnet die Aktion wie folgt:\n",
    "\n",
    "- Die Wahrscheinlichkeiten `probs` werden mit dem `PolicyNet` (hier als `self.net` erreichbar) berechnet,\n",
    "- es wird mit `torch.distrib.Categorial` eine passende Wahrscheinlichkeitsverteilung erzeugt und mit `sample()` eine Aktion ausgewürfelt,\n",
    "- in `self.memory` wird der Logarithmus der Wahrscheinlichkeit (`m.log_prob(acttion)`) gespeichert (für das spätere Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2f833-bad3-44af-8d7f-9d915a3ecdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "    def __call__(self, observation):\n",
    " \n",
    "        probs = ### YOUR CODE HERE\n",
    "        m = ### YOUR CODE HERE\n",
    "        action = ### YOUR CODE HERE\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action)))\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        \n",
    "    def discount_rewards(self, r):\n",
    "        discounted = torch.zeros(len(r))\n",
    "        summe = 0\n",
    "        for t in reversed(range(0, len(r))):\n",
    "            summe = summe * self.gamma + r[t]\n",
    "            discounted[t] = summe\n",
    "        return discounted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc829a48-1bf5-451e-bb1b-582c4ccc092f",
   "metadata": {},
   "source": [
    "### Update des Modells\n",
    "\n",
    "Das Training findet jeweils am Ende einer Spielepisode statt:\n",
    "\n",
    "- Zunächst werden die *diskontierten Belohnungen* berechnet,\n",
    "- diese werden so skaliert, dass sie normalverteilt sind.\n",
    "- Der Verlust der Policy ergibt sich als `- reward * log_prob`,\n",
    "- mit der Summe der Verluste wird ein `optimizer.step()` durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658600f-c46a-4684-9e11-64154215bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                policy_losses.append(### YOUR CODE HERE ###)\n",
    "                \n",
    "            loss = torch.stack(policy_losses).sum()\n",
    "            ### YOUR CODE HERE\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb88408-d74a-4c1f-a8a0-00ad3f065052",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SimplePolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e55ffcf-7ab6-4107-86a9-bdbd7ec6aa0b",
   "metadata": {},
   "source": [
    "## Aufgabe 1.1: Actor-Critic-Modell\n",
    "\n",
    "Für das Actor-Critic-Modell erhält unser Netzwerk einen weiteren Ausgang `critic`, der eine Schätzung des Werts eines Zustands liefern soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e08835-f835-4e51-b342-673eead6e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=128, n_actions=2):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        probs = self.policy(x)\n",
    "        value = self.critic(x)\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaefd1d-dd75-4da8-88fd-94d97a03116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "    \n",
    "class ACPolicy:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        # Two possible actions 0, 1\n",
    "        self.ACTIONS = [0, 1]       \n",
    "        self.net = ActorCriticNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc9e94-a17a-451b-a98d-0f016be825cb",
   "metadata": {},
   "source": [
    "### Strategie\n",
    "\n",
    "Die Strategie berechnet nun auch den Wert des Zustands mithilfe des Netzwerks und speichert diesen für das Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33e9a7-a31e-4a5c-9032-70603a502495",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __call__(self, observation):\n",
    " \n",
    "        probs, value = ### YOUR CODE HERE\n",
    "        m = ### YOUR CODE HERE\n",
    "        action = ### YOUR CODE HERE\n",
    "        \n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        \n",
    "        return self.ACTIONS[action.item()]\n",
    "        \n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfa50e-f0d1-4ec6-982a-bd5828cc9262",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Die Verlustfunktion besteht nun aus zwei Teilen:\n",
    "\n",
    "1. Der `policy_loss` summiert `log_prob * advantage`, wobei `advantage = discounted_reward - value` ist. \n",
    "   Diese Differenz wird auch als *temporal difference* bezeichnet.\n",
    "2. Der `value_loss` summiert die Abweichung zwischen `value` und diskontiertem Reward. \n",
    "   Dabei wird meistens `F.smooth_l1_loss` verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f3546-3d3f-43cc-944f-30cf9a000e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "                \n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "            \n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                ### YOUR CODE HERE\n",
    "               \n",
    "            self.optimizer.zero_grad()\n",
    "            ### YOUR CODE HERE\n",
    "            \n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "    \n",
    "    \n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "        \n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "                    'games': self.games,\n",
    "                    'model_state_dict': self.net.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'mean_reward': self.mean_reward}, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6c96-3282-4238-868a-f420c4aed689",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ACPolicy()\n",
    "play_game(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ff7d-c9ef-48b1-bd6f-79f2a7a5a53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
