{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Generative Adversarial Networks\n",
    "\n",
    "In dieser Aufgabe wollen wir ein *DCGAN* auf Basis des CIFAR10 Datensatzes trainieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Bilddaten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "#batch_size=64\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "dataroot = \"/home/shared-data/celeba/images\"\n",
    "dataset = datasets.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.CenterCrop(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=10)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "\n",
    "In der nächsten Zelle wählen Sie Hyperparameter wie die Dimension des latenten Raums oder die Zahl der Features für Generator und Diskriminator.\n",
    "\n",
    "Bei GANs ist es ungünstig, wenn der Diskriminator deutlich besser ist als der Generator. Die Zahl der Features bzw. das Verhältnis zwischen Generator und Diskriminator kann hier einen Einfluss haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Size of latent vector\n",
    "nz = 100\n",
    "\n",
    "# Filter size of generator\n",
    "ngf = 64\n",
    "\n",
    "# Filter size of discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Output image channels\n",
    "nc = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisierung der Gewichte\n",
    "\n",
    "Bei GANs ist die Initialisierung der Gewichte in den Netzwerken wichtig. Die folgende Funktion basiert auf Best Practices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_inititialisation(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator-Netzwerk\n",
    "\n",
    "Definieren Sie hier das Generator-Netzwerk wie folgt:\n",
    "\n",
    "- Verwenden Sie fünf `ConvTranspose2d` Schichten. Dabei sollen die Kanäle von `nz -> 8*ngf -> 4*ngf -> 2*ngf -> ngf -> nc` gewechselt werden.\n",
    "- Die Kernelgröße ist jeweils 4.\n",
    "- Die erste Schicht hat Stride 1 und Padding 0, die folgenden Schichten Stride 2 und Padding 1.\n",
    "- Auf die ersten vier Schichten folgen jeweils `BatchNorm2d` und `ReLU`. \n",
    "- Die letzte Schicht verwendet als Aktivierungsfunktion `nn.Tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _net_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_net_generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "         ### YOUR CODE HERE\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "net_generator = _net_generator()\n",
    "net_generator.apply(weights_inititialisation)\n",
    "print(net_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diskriminator-Netzwerk\n",
    "\n",
    "Definieren Sie hier das Diskriminator-Netzwerk wie folgt:\n",
    "\n",
    "- Verwenden Sie fünf `Conv2d` Schichten. Dabei sollen die Kanäle von `nc -> ndf -> 2*ndf -> 4*ndf -> 8*ndf` gewechselt werden.\n",
    "- Die Kernelgröße ist jeweils 4.\n",
    "- Die ersten vier Schichten haben Stride 2 und Padding 1, die letzte Schicht Stride 1 und Padding 0.\n",
    "- Auf die ersten vier Schichten folgen jeweils `BatchNorm2d` und `LeakyReLU(0.2, inplace=True)`. \n",
    "- Die letzte Schicht verwendet als Aktivierungsfunktion `nn.Sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _net_discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_net_discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            ### YOUR CODE HERE\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "net_discriminator = _net_discriminator()\n",
    "net_discriminator.apply(weights_inititialisation)\n",
    "print(net_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition der Verlustfunktion und einiger Hilfswerte\n",
    "\n",
    "Als Verlust verwenden wir die binäre Kreuzentropie (Warum?).\n",
    "\n",
    "Der `fixed_noise` dient dazu, während des Trainings Bilder mit immer gleichen Startvektoren zu erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = ### YOUR CODE HERE\n",
    "\n",
    "input = torch.FloatTensor(batch_size, 3, img_size, img_size)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net_discriminator.cuda()\n",
    "    net_generator.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition des Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "optimizer_discriminator = optim.Adam(net_discriminator.parameters(), lr, betas=(beta1, 0.95))\n",
    "optimizer_generator = optim.Adam(net_generator.parameters(), lr, betas=(beta1, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsschleife \n",
    "\n",
    "Die Trainingsschleife trainiert je Epoche\n",
    "\n",
    "- zunächst den Diskriminator, der dazu je einen \"echten\" und einen \"fake\" Batch verarbeitet,\n",
    "- dann den Generator. Hierbei ist zu beachten, dass er möglichst \"echte\" Bilder erzeugen soll und der Verlust daher gegen \"real\" Label gemessen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "\n",
    "with tqdm(range(num_epochs)) as pbar: \n",
    "    for epoch in pbar:\n",
    "        # For each batch in the dataloader\n",
    "        for i, (data, label) in enumerate(tqdm(dataloader), 0):\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            net_discriminator.zero_grad()\n",
    "            # Format batch\n",
    "            real_cpu = data.to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            output = net_discriminator(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = net_generator(noise)\n",
    "            label.fill_(fake_label)\n",
    "            # Classify all fake batch with D\n",
    "            output = net_discriminator(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label)\n",
    "            # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Compute error of D as sum over the fake and the real batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            net_generator.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = net_discriminator(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizer_generator.step()\n",
    "\n",
    "            # Output training stats\n",
    "            if i % 50 == 0:\n",
    "                pbar.set_postfix({ \"Loss_D\": errD.item(), \"Loss_G\": errG.item(), \"D(x)\": D_x, \"D(G(z1))\": D_G_z1, \"D(G(z2))\": D_G_z2})\n",
    "            #    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "            #          % (epoch, num_epochs, i, len(dataloader),\n",
    "            #             errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake = net_generator(fixed_noise).detach().cpu()\n",
    "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "            iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darstellung der Ergebnisse\n",
    "\n",
    "Die folgenden beiden Zellen plotten die Lernkurven und animieren die aus dem `fixed_noise` generierten Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
