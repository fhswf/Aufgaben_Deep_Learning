{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5d6e0d-0454-47e3-9c33-3aaa80e06693",
   "metadata": {},
   "source": [
    "# Erzeugung von Bildunterschriften mitt einem Encoder-Decoder-Netz\n",
    "\n",
    "Das Microsoft **C**ommon **O**bjects in **CO**ntext (MS COCO) Dataset wird normalerweise zum Training von Modellen zur Objekt- und Szenenerkennung verwendet. \n",
    "\n",
    "![COCO Sample Image](https://cocodataset.org/images/coco-examples.jpg)\n",
    "\n",
    "Der Datensatz enthält unter anderem gut 200.000 Bilder mit von Menschen erzeugten Bildbeschreibungen. Diese wollen wir nutzen, um ein Encoder-Decoder-Modell zur Erzeugung von Bildbeschreibungen zu erzeugen.\n",
    "\n",
    "Dabei kombinieren wir ein vortrainiertes **ResNeXt-Modell** als Encoder mit einem **RNN** als Decoder. \n",
    "\n",
    "*Hinweis: Mittlerweile gibt es mit der Transformer-Architektur sowohl auf der Encoder- als auch der Decoder-Seite eine leistungsfähigere Architektur. Dies wäre ein mögliches Thema für eine Hausarbeit. Auch der umgekehrte Weg – die Erzeugung von Bildern zu Beschreibungen mithilfe von **Strabele Diffussdion** und **CLIP** wäre ein spannendes Thema.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba7d91-e500-4acd-9f32-0d74a0b9dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/salaniz/pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833e838-14e0-46bf-8a75-a9c8de89d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed8233-34be-4293-8ebf-99650f563614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67629f-1282-4a57-833c-eefe02f6a958",
   "metadata": {},
   "source": [
    "## Laden des Datensatzes\n",
    "\n",
    "Den Datensatz finden Sie zentral unter `/data/coco`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e342f61-7cea-444d-bb71-708780ae7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize COCO API for instance annotations\n",
    "dataDir = '/data/coco'\n",
    "dataType = 'val2014'\n",
    "instances_annFile = os.path.join(dataDir, f'annotations/instances_{dataType}.json')\n",
    "coco = COCO(instances_annFile)\n",
    "\n",
    "# initialize COCO API for caption annotations\n",
    "captions_annFile = os.path.join(dataDir, f'annotations/captions_{dataType}.json')\n",
    "coco_caps = COCO(captions_annFile)\n",
    "\n",
    "# get image ids \n",
    "ids = list(coco.anns.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2db086-b112-4cb3-83ea-4ca3268d2fc0",
   "metadata": {},
   "source": [
    "Die folgende Zelle lädt ein Beispielbild samt der zugehörigen Beschreibungen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69dd4a-83c7-424b-a4b1-81d2789f2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# pick a random image and obtain the corresponding URL\n",
    "ann_id = np.random.choice(ids)\n",
    "img_id = coco.anns[ann_id]['image_id']\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "url = img['coco_url']\n",
    "\n",
    "# print URL and visualize corresponding image\n",
    "print(url)\n",
    "I = io.imread(url)\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "plt.show()\n",
    "\n",
    "# load and display captions\n",
    "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
    "anns = coco_caps.loadAnns(annIds)\n",
    "coco_caps.showAnns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a4b7e-852a-4a93-bf4a-42b9e578ae81",
   "metadata": {},
   "source": [
    "## Tokenisierung und Vektorisierung der Texte\n",
    "\n",
    "Wir erstellen ein Vokabular für die Beschreibungen und fügen dabei drei spezielle Token ein:\n",
    "\n",
    "- `<start>` als Markierung für den Start des Textes,\n",
    "- `<end>` als Markierung für das Ende des Textes,\n",
    "- `<pad>` als Füllzeichen (*Padding*),\n",
    "- `<unk>` als Markierung für unbekannte Worte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccf63a-b7e1-4a77-a21d-71ca2dce63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import os.path\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        vocab_threshold,\n",
    "        vocab_file=\"./vocab.pkl\",\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        pad_word=\"<pad>\",\n",
    "        vocab_from_file=False,\n",
    "        coco_annotations_file=\"/data/coco/annotations/captions_train2014.json\"):\n",
    "      \n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Paramters:\n",
    "          vocab_threshold: Minimum word count threshold.\n",
    "          vocab_file: File containing the vocabulary.\n",
    "          start_word: Special word denoting sentence start.\n",
    "          end_word: Special word denoting sentence end.\n",
    "          unk_word: Special word denoting unknown words.\n",
    "          annotations_file: Path for train annotation file.\n",
    "          vocab_from_file: If False, create vocab from scratch & override any\n",
    "                           existing vocab_file. If True, load vocab from from\n",
    "                           existing vocab_file, if it exists.\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.pad_word = pad_word\n",
    "        self.coco_annotations_file = coco_annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Load the vocabulary from file or build it from scratch.\"\"\"\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, \"rb\") as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print(\"Vocabulary successfully loaded from vocab.pkl file!\")\n",
    "        else:\n",
    "            print(\"Building vocabulary: {}, {}\".format(os.path.exists(self.vocab_file), self.vocab_from_file))\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, \"wb\") as f:\n",
    "                pickle.dump(self, f)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Populate the dictionaries for converting tokens to integers \n",
    "        (and vice-versa).\"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.pad_word)\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize the dictionaries for converting tokens to integers\n",
    "        (and vice-versa).\"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"Loop over training captions and add all tokens to the vocabulary \n",
    "        that meet or exceed the threshold.\"\"\"\n",
    "        coco = COCO(self.coco_annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(tqdm(ids, desc=\"tokenizing captions\")):\n",
    "            caption = str(coco.anns[id][\"caption\"])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "        words = [word for word, cnt in counter.items()\n",
    "                 if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9fc9a-9fef-4a4f-b3b5-f36395426b42",
   "metadata": {},
   "source": [
    "## Erstellen eines `Dataset`\n",
    "\n",
    "Die folgende Klasse kapselt den COCO Datensatz als PyTorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f250c5-be09-41f1-854a-d82d6383d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "class CocoDataset(data.Dataset):\n",
    "\n",
    "    coco_paths = { \"train\": [ \"/data/coco/annotations/captions_train2014.json\", \"/data/coco/images/train2014/\" ],\n",
    "                   \"val\":  [ \"/data/coco/annotations/captions_val2014.json\", \"/data/coco/images/val2014/\" ],\n",
    "                   \"test\": [ \"/data/coco/annotations/image_info_test2014.json\", \"/data/coco/images/test2014/\" ]\n",
    "                 } \n",
    "\n",
    "    def __init__(self, transform, mode, batch_size, vocabulary):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = vocabulary\n",
    "        self.coco_annotations_file = self.coco_paths[mode][0]\n",
    "        self.coco_img_folder = self.coco_paths[mode][1]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            self.coco = COCO(self.coco_annotations_file)\n",
    "            self.coco_ids = list(self.coco.anns.keys())\n",
    "              \n",
    "            print(\"Obtaining caption lengths...\")\n",
    "            coco_tokens = [nltk.tokenize.word_tokenize(\n",
    "                          str(self.coco.anns[self.coco_ids[index]][\"caption\"]).lower())\n",
    "                            for index in tqdm(np.arange(len(self.coco_ids)))]\n",
    " \n",
    "            self.caption_lengths = [len(token) for token in coco_tokens]\n",
    "            self.max_length = max(self.caption_lengths)\n",
    "\n",
    "        elif self.mode == \"val\":\n",
    "            self.coco = COCO(self.coco_annotations_file)\n",
    "            self.coco_ids = list(self.coco.imgs.keys())\n",
    " \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Obtain image and caption if in training or validation mode\n",
    "        if self.mode == \"train\":\n",
    "            ann_id = self.coco_ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "            image = Image.open(os.path.join(self.coco_img_folder, path)).convert(\"RGB\")\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            \n",
    "            image = self.transform(image)\n",
    "\n",
    "            orig = caption\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            length = len(caption)\n",
    "            for i in range(len(caption), self.max_length+2):\n",
    "                caption.append(self.vocab(self.vocab.pad_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # Return pre-processed image and caption tensors\n",
    "            return image, caption, length, ann_id\n",
    "\n",
    "        elif self.mode == \"val\":\n",
    "            img_id = self.coco_ids[index]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "            image = Image.open(os.path.join(self.coco_img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            return image, torch.empty(1), torch.empty(1), img_id\n",
    "\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.coco_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8277a-0ffc-49c3-8a2d-6564b7d8efee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d47640-0c7f-473c-80e0-91b336f4d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataLoader(data.DataLoader):\n",
    "    vocab = None\n",
    "\n",
    "    def __init__(self,\n",
    "                transform,\n",
    "               mode=\"train\",\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file=\"./vocab.pkl\",\n",
    "               start_word=\"<start>\",\n",
    "               end_word=\"<end>\",\n",
    "               unk_word=\"<unk>\",\n",
    "               pad_word=\"<pad>\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0):\n",
    "        \"\"\"Return the data loader.\n",
    "        Parameters:\n",
    "        transform: Image transform.\n",
    "        mode: One of \"train\", \"val\" or \"test\".\n",
    "        batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "        vocab_threshold: Minimum word count threshold.\n",
    "        vocab_file: File containing the vocabulary. \n",
    "        start_word: Special word denoting sentence start.\n",
    "        end_word: Special word denoting sentence end.\n",
    "        unk_word: Special word denoting unknown words.\n",
    "        vocab_from_file: If False, create vocab from scratch & override any \n",
    "                         existing vocab_file. If True, load vocab from from\n",
    "                         existing vocab_file, if it exists.\n",
    "        num_workers: Number of subprocesses to use for data loading \n",
    "        cocoapi_loc: The location of the folder containing the COCO API: \n",
    "                     https://github.com/cocodataset/cocoapi\n",
    "        \"\"\"\n",
    "    \n",
    "        assert mode in [\"train\", \"val\", \"test\"], \"mode must be one of 'train', 'val' or 'test'.\"\n",
    "\n",
    "        if self.vocab is None:\n",
    "            if vocab_from_file == False: \n",
    "                assert mode == \"train\", \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "            self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "                                    end_word, unk_word, pad_word, vocab_from_file)\n",
    "     \n",
    "        # COCO caption dataset\n",
    "        self.coco_dataset = CocoDataset(transform=transform,\n",
    "                          mode=mode,\n",
    "                          batch_size=batch_size,\n",
    "                          vocabulary=self.vocab)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            # Calculating overall maximum of caption length (needed for padding)\n",
    "            print(f\"Maximum caption length: {self.coco_dataset.max_length}\")\n",
    "            \n",
    " \n",
    "        dataset = self.coco_dataset\n",
    "        super().__init__(dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055dda9-86e8-4791-b396-fe618859634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Obtain the data loader.\n",
    "train_loader = MyDataLoader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=10,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)\n",
    "\n",
    "transform_val = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),                      # get 224x224 crop from the center\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_loader = MyDataLoader(transform=transform_val,\n",
    "                        mode='val',\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=5,\n",
    "                        vocab_threshold=vocab_threshold,\n",
    "                        vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d60a1-14f9-45b8-b722-6efe95d49435",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Die folgende Klasse kapselt den Training- und Evaluationsprozess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dc928-86f3-4775-afc2-d1f6a3ee9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"The Trainer encapsulates the model training process.\"\"\"\n",
    "\n",
    "    def __init__(self, train_loader, val_loader, encoder, decoder, optimizer, device=torch.device('cuda'), criterion=None, start_epoch=0, rounds=1):\n",
    "        \"\"\"Initialize the Trainer state. This includes loading the model data if start_epoch > 0.\"\"\"\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.epoch = start_epoch\n",
    "        self.rounds = rounds\n",
    "        self.current_state_file = os.path.join('./models', 'current-model.pkl')\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.vocab = self.train_loader.vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        if criterion is None:\n",
    "            pad_idx = self.vocab.word2idx['<pad>']\n",
    "            self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "        else: \n",
    "            self.criterion = criterion\n",
    "        \n",
    "        self.map_location = self.device\n",
    "        self.criterion.to(self.device)\n",
    "        self.encoder.to(self.device)\n",
    "        self.decoder.to(self.device)\n",
    "\n",
    "        self.cider = []\n",
    "\n",
    "        if self.epoch > 0:\n",
    "            self.load()\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load the model output of an epoch.\"\"\"\n",
    "        checkpoint = torch.load(self.current_state_file, map_location=self.map_location)\n",
    "\n",
    "        # Load the pre-trained weights\n",
    "        self.encoder.load_state_dict(checkpoint['encoder'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.cider = checkpoint['cider']\n",
    "        print('Successfully loaded epoch {}'.format(self.epoch))\n",
    "\n",
    "    def save_as(self, file_name):\n",
    "        \"\"\"Save the training state in a pickle file.\n",
    "        The following values are saved: \n",
    "        - encoder parameter, \n",
    "        - decoder parameters,\n",
    "        - optimizer state, \n",
    "        - current epoch,\n",
    "        - list of CIDEr scores from the evaluation of past epochs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_name : str\n",
    "            Name of the file to save.\n",
    "        \"\"\"\n",
    "        torch.save({\"encoder\": self.encoder.state_dict(),\n",
    "                    \"decoder\": self.decoder.state_dict(),\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"cider\": self.cider,\n",
    "                    \"epoch\": self.epoch\n",
    "                   }, file_name)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save the training state in a pickle file.\n",
    "        \n",
    "        The following values are saved: \n",
    "        - encoder parameter, \n",
    "        - decoder parameters,\n",
    "        - optimizer state, \n",
    "        - current epoch,\n",
    "        - list of CIDEr scores from the evaluation of past epochs.\n",
    "        \"\"\"\n",
    "        self.save_as(os.path.join(\"./models\", \"current-model.pkl\"))\n",
    "        self.save_as(os.path.join(\"./models\", \"epoch-model-{}.pkl\".format(self.epoch)))\n",
    "\n",
    "    def clean_sentence(self, word_idx_list):\n",
    "        \"\"\"Take a list of word ids and a vocabulary from a dataset as inputs\n",
    "        and return the corresponding sentence as a single Python string.\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_idx_list : list\n",
    "            List of word indices, i.e. embedded words.\n",
    "        \"\"\"\n",
    "        sentence = []\n",
    "        for i in range(len(word_idx_list)):\n",
    "            vocab_id = word_idx_list[i]\n",
    "            word = self.vocab.idx2word[vocab_id]\n",
    "            if word == self.vocab.end_word:\n",
    "                break\n",
    "            if word != self.vocab.start_word:\n",
    "                sentence.append(word)\n",
    "        sentence = \" \".join(sentence)\n",
    "        return sentence\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model for one epoch using the provided parameters. Return the epoch's average train loss.\"\"\"\n",
    "\n",
    "        # Switch to train mode\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "        # Keep track of train loss\n",
    "        total_loss = 0\n",
    "\n",
    "        # Start time for every 100 steps\n",
    "        start_train_time = time.time()\n",
    "        i_step = 0\n",
    "    \n",
    "        # Obtain the batch\n",
    "        pbar = tqdm(self.train_loader)\n",
    "        pbar.set_description('training epoch {}'.format(self.epoch))\n",
    "        for batch in pbar:\n",
    "            i_step += 1\n",
    "            images, captions, lengths = batch[0], batch[1], batch[2]\n",
    "             \n",
    "            images = images.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            lengths = lengths.to(self.device)\n",
    "\n",
    "            # Pass the inputs through the CNN-RNN model\n",
    "            features = self.encoder(images)\n",
    "            outputs = self.decoder(features, captions, lengths)\n",
    "\n",
    "            # Calculate the batch loss\n",
    "            # Flatten batch dimension\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            captions = captions.view(-1)\n",
    "\n",
    "            loss = self.criterion(outputs, captions)\n",
    "\n",
    "            # Zero the gradients. Since the backward() function accumulates \n",
    "            # gradients, and we don’t want to mix up gradients between minibatches,\n",
    "            # we have to zero them out at the start of a new minibatch\n",
    "            self.optimizer.zero_grad()\n",
    "            # Backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "            # Update the parameters in the optimizer\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pbar.set_postfix(last=loss.item(), avg=total_loss/i_step)\n",
    "            \n",
    "        self.epoch += 1\n",
    "        self.save()\n",
    "\n",
    "        return total_loss / i_step\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model for one epoch using the provided parameters. \n",
    "           Return the epoch's average CIDEr score.\"\"\"\n",
    "\n",
    "        # Switch to validation mode\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "        cocoRes = COCO()\n",
    "        anns = []\n",
    "\n",
    "        # Disable gradient calculation because we are in inference mode\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader)\n",
    "            pbar.set_description('evaluating epoch {}'.format(self.epoch));\n",
    "            for batch in pbar:\n",
    "                images, img_id = batch[0], batch[3]\n",
    "\n",
    "                images = images.to(self.device)\n",
    "\n",
    "                # Pass the inputs through the CNN-RNN model\n",
    "                features = encoder(images).unsqueeze(1)\n",
    "                for i in range(img_id.size()[0]):\n",
    "                    slice = features[i].unsqueeze(0)\n",
    "                    outputs = decoder.sample_beam_search(slice)\n",
    "                    sentence = self.clean_sentence(outputs[0])\n",
    "                    id = img_id[i].item()\n",
    "                    #print('id: {}, cap: {}'.format(id, sentence))\n",
    "                    anns.append({'image_id': id, 'caption': sentence})\n",
    "             \n",
    "        for id, ann in enumerate(anns):\n",
    "            ann['id'] = id\n",
    "    \n",
    "        cocoRes.dataset['annotations'] = anns\n",
    "        cocoRes.createIndex()\n",
    "\n",
    "        cocoEval = COCOEvalCap(self.val_loader.coco_dataset.coco, cocoRes)\n",
    "        imgIds = set([ann['image_id'] for ann in cocoRes.dataset['annotations']])\n",
    "        cocoEval.params['image_id'] = imgIds\n",
    "        cocoEval.evaluate()\n",
    "        cider = cocoEval.eval['CIDEr']\n",
    "        old_max = 0\n",
    "        if len(self.cider) > 0:\n",
    "            old_max = max(self.cider)\n",
    "\n",
    "        if len(self.cider) < self.epoch:\n",
    "            self.cider.append(cider)\n",
    "        else:\n",
    "            self.cider[self.epoch-1] = cider\n",
    "        self.save()\n",
    "        print(\"DEBUG: self.epoch: {}, self.cider: {}\".format(self.epoch, self.cider))\n",
    "        if cider > old_max:\n",
    "            print('CIDEr improved: {:.2f} => {:.2f}'.format(old_max, cider))\n",
    "            self.save_as(os.path.join(\"./models\", \"best-model.pkl\"))\n",
    "\n",
    "        return self.cider[self.epoch-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1fa91-2939-4ab0-8254-36835ea2028d",
   "metadata": {},
   "source": [
    "## Encoder & Decoder Netzwerke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b0d65-08bd-4e63-b615-cee4da7cb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        features = self.bn(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"Decoder RNN based on 2-layer GRU model.\"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=2, dropout=0.3):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        # Store length of the padded captions - we need it re-pad the packed output sequences \n",
    "        total_length = captions.size(1)\n",
    "        \n",
    "        reduced_lengths = lengths\n",
    "        embeddings = self.embed(captions)\n",
    "\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "\n",
    "        # pack padded input sequences\n",
    "        inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs, reduced_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.gru(inputs)\n",
    "        # pad sequences again using the stored length of the padded captions\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, total_length=total_length)\n",
    "        outputs = self.linear(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"Accept a pre-processed image tensor (inputs) and return predicted \n",
    "        sentence (list of tensor ids of length max_len). This is the greedy\n",
    "        search approach.\n",
    "        \"\"\"\n",
    "        sampled_ids = []\n",
    "        for i in range(max_len):\n",
    "            hiddens, states = self.gru(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            # Get the index (in the vocabulary) of the most likely integer that\n",
    "            # represents a word\n",
    "            predicted = outputs.argmax(1)\n",
    "            sampled_ids.append(predicted.item())\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        return sampled_ids\n",
    "\n",
    "    def sample_beam_search(self, inputs, states=None, max_len=20, beam_width=5):\n",
    "        \"\"\"Accept a pre-processed image tensor and return the top predicted \n",
    "        sentences. This is the beam search approach.\n",
    "        \"\"\"\n",
    "        # Top word idx sequences and their corresponding inputs and states\n",
    "        idx_sequences = [[[], 0.0, inputs, states]]\n",
    "        for _ in range(max_len):\n",
    "            # Store all the potential candidates at each step\n",
    "            all_candidates = []\n",
    "            # Predict the next word idx for each of the top sequences\n",
    "            for idx_seq in idx_sequences:\n",
    "                hiddens, states = self.gru(idx_seq[2], idx_seq[3])\n",
    "                outputs = self.linear(hiddens.squeeze(1))\n",
    "                # Transform outputs to log probabilities to avoid floating-point \n",
    "                # underflow caused by multiplying very small probabilities\n",
    "                log_probs = F.log_softmax(outputs, -1)\n",
    "                top_log_probs, top_idx = log_probs.topk(beam_width, 1)\n",
    "                top_idx = top_idx.squeeze(0)\n",
    "                # create a new set of top sentences for next round\n",
    "                for i in range(beam_width):\n",
    "                    next_idx_seq, log_prob = idx_seq[0][:], idx_seq[1]\n",
    "                    next_idx_seq.append(top_idx[i].item())\n",
    "                    log_prob += top_log_probs[0][i].item()\n",
    "                    # Indexing 1-dimensional top_idx gives 0-dimensional tensors.\n",
    "                    # We have to expand dimensions before embedding them\n",
    "                    inputs = self.embed(top_idx[i].unsqueeze(0)).unsqueeze(0)\n",
    "                    all_candidates.append([next_idx_seq, log_prob, inputs, states])\n",
    "            # Keep only the top sequences according to their total log probability\n",
    "            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "            idx_sequences = ordered[:beam_width]\n",
    "        return [idx_seq[0] for idx_seq in idx_sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0055-1d13-4a43-aaf2-9484b366889b",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "\n",
    "Trainieren Sie mit der folgenden Zelle das Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9662dcb-30d0-4f8d-a6a6-86342087f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for the training variables\n",
    "batch_size = 512        # batch size\n",
    "vocab_threshold = 5     # minimum word count threshold\n",
    "vocab_from_file = True   # if True, load existing vocab file\n",
    "embed_size = 512        # dimensionality of image and word embeddings\n",
    "hidden_size = 512       # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 10         # number of training epochs\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The size of the vocabulary\n",
    "vocab_size = len(train_loader.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters()) + list(encoder.resnet.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(params=params, lr=0.001, weight_decay=0.05, amsgrad=True)\n",
    "\n",
    "trainer = Trainer(train_loader, val_loader, encoder, decoder, optimizer, device=device)\n",
    "\n",
    "if not os.path.exists(trainer.current_state_file):\n",
    "    trainer.train()\n",
    "trainer.load()\n",
    "\n",
    "# if cider is missing for current epoch, evaluate first\n",
    "if len(trainer.cider) < trainer.epoch:\n",
    "    print('Epoch {} not yet evaluated'.format(trainer.epoch))\n",
    "    trainer.evaluate()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    trainer.train()\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a0e14-17bb-4597-83be-c3273387472a",
   "metadata": {},
   "source": [
    "## Erzeugen von Bildunterschriften\n",
    "\n",
    "Zum Erzeugen von Bildunterschriften generieren wir zunächst Features und bestimmen dann mittels Beam-Search die besten Kandidaten für die Beschreibu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4f749-25ad-442d-9b97-b7bc27aefc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample batch\n",
    "for imgs, _, _, img_ids in val_loader:\n",
    "    break\n",
    "    \n",
    "imgs = imgs[0].unsqueeze(0).to(device)\n",
    "\n",
    "img_id = img_ids[0].item()\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "url = img['coco_url']\n",
    "\n",
    "# print URL and visualize corresponding image\n",
    "print(url)\n",
    "I = io.imread(url)\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "plt.show()\n",
    "\n",
    "# Calculate features\n",
    "\n",
    "vocab = val_loader.vocab\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encoder(imgs).unsqueeze(1)\n",
    "\n",
    "# Generate caption with decoder.sample_beam_search()\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb2cf7-d63e-4675-bce8-69444eff4e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
